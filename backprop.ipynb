{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "backprop.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hq1AS2ym_fP0"
      },
      "source": [
        "Let us again attempt to learn the XOR function using the same MLP network,  this time starting with random initial weights and using back-propogation with simple gradient descent.\n",
        "\n",
        "The error functions for each neuron are\n",
        "\\begin{eqnarray}\n",
        "\\Delta_1^{(2)} &=& {\\partial{J} \\over \\partial a_1^{(2)}}\\,, \\\\\n",
        "\\Delta_1^{(1)} &=&  \\Delta_1^{(2)} W_{11}^{(2)}   \\Theta ( z_1^{(1)} )  \\\\\n",
        "\\Delta_2^{(1)} &=&  \\Delta_1^{(2)} W_{21}^{(2)}  \\Theta ( z_2^{(1)} )  \\,,\n",
        "\\end{eqnarray}\n",
        "since the Heaviside step function $\\Theta$ is the derivative of the ReLU activation function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F4ilVZD2J7u",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xmxhuihui/Self-driving-Car-Project/blob/master/backprop.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LxKaSAFOxPZz",
        "outputId": "dda1c738-a360-41be-b120-1d51316d912a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-a5a010e3ea63>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    [[Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7udklsBw_TOu",
        "outputId": "6e2f0b62-6723-4a82-abd4-5edb087ac6f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "np.random.seed(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-9481a407e131>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f_EC8JkUAHwO",
        "colab": {}
      },
      "source": [
        "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "print(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JzYShJU4ANmR",
        "colab": {}
      },
      "source": [
        "Y = np.array([[0], [1], [1], [0]])\n",
        "print(Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KFxtTE3DGs09",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1 + np.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E3O5LePZVjzp"
      },
      "source": [
        "Define the MLP model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TuU43LLm_Aj1",
        "colab": {}
      },
      "source": [
        "class MLP(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    # Initialise with random weights\n",
        "    self.weights_1 = 0.1 * np.random.normal(size=(3,2))\n",
        "    self.weights_2 = 0.1 * np.random.normal(size=(3,1))\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Do a forward pass\n",
        "    if len(x.shape) == 1:\n",
        "      # Single example, so add a batch dimension of 1\n",
        "      x = np.expand_dims(x, axis=0)\n",
        "    # Hidden layer \n",
        "    z_1 = np.matmul(np.hstack((np.ones(shape=(x.shape[0], 1)), x)), self.weights_1)\n",
        "    # Apply ReLU activation function\n",
        "    a_1 = np.maximum(z_1, 0)\n",
        "    # Output layer\n",
        "    z_2 = np.matmul(np.hstack((np.ones(shape=(a_1.shape[0], 1)), a_1)), self.weights_2)\n",
        "    # Linear activation \n",
        "    a_2 = sigmoid(z_2)\n",
        "    return z_1, a_1, z_2, a_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dSTxT91KjAe4"
      },
      "source": [
        "Push the true solution through the network and check it gives zero loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GGnwcJdzUs1l",
        "colab": {}
      },
      "source": [
        "m = MLP()\n",
        "m.weights_1 = np.array([[0, -1], [1,1], [1,1]], dtype=np.float)\n",
        "m.weights_2 = np.array([[0], [1], [-2]], dtype=np.float)\n",
        "z_1, a_1, z_2, a_2 = m.forward(X)\n",
        "print(0.25 * np.sum((a_2 - Y)**2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kkRYVpaF9pQc",
        "colab": {}
      },
      "source": [
        "num_epochs = 10001\n",
        "learning_rate = 0.01"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eaq2lIzwqoPY"
      },
      "source": [
        "Update random initial weights and biases by back-prop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kaRG-DXJjiD1",
        "colab": {}
      },
      "source": [
        "m = MLP()\n",
        "loss_history = []\n",
        "weights_1_history = []\n",
        "weights_2_history = []\n",
        "for epoch in range(num_epochs):\n",
        "  # Do forward pass\n",
        "  z_1, a_1, z_2, a_2 = m.forward(X)\n",
        "  #loss = 0.25 * np.sum((a_2 - Y)**2)\n",
        "  loss=-np.sum((Y*np.log(a_2)+(np.ones(Y.shape)-Y)*np.log(np.ones(a_2.shape)-a_2)))\n",
        "  loss_history.append(loss)\n",
        "  if epoch % 100 == 0:\n",
        "    print(epoch, loss)\n",
        "  # Delta_2 has shape(4, 1), the first dimension being the batch dimension\n",
        "  #delta_2 = 0.5 * ( a_2 - Y)\n",
        "  delta_2=-(Y/a_2+(np.ones(Y.shape)-Y)/(a_2-np.ones(a_2.shape)))\n",
        "  g_prime_1 = np.heaviside(z_1, 0)\n",
        "  # Delta_1 has shape (4, 2)\n",
        "  delta_1 = np.matmul(delta_2, m.weights_2[1:3, :].T) * g_prime_1\n",
        "  # Biases of layers connecting input and hidden layers\n",
        "  m.weights_1[0, :] -= learning_rate * np.sum(delta_1[:, :], axis=0)\n",
        "  # Weights of layers connecting input and hidden layers \n",
        "  m.weights_1[1:3, :] -= learning_rate * np.matmul(X.T, delta_1)\n",
        "  # Biases of layers connecting hidden and output layers\n",
        "  m.weights_2[0, 0] -= learning_rate * np.sum(delta_2[:, :], axis=0)\n",
        "  # Weights of layers connecting hidden and output layers\n",
        "  m.weights_2[1:3, 0:1] -= learning_rate * np.matmul(a_1.T, delta_2)\n",
        "  weights_1_history.append(np.copy(m.weights_1))\n",
        "  weights_2_history.append(np.copy(m.weights_2))\n",
        "loss_history = np.array(loss_history)\n",
        "weights_1_history = np.array(weights_1_history)\n",
        "weights_2_history = np.array(weights_2_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l_UNqVI12Fez",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(15, 5))\n",
        "ax = plt.subplot(2, 5, 1)\n",
        "ax.plot(loss_history[:])\n",
        "ax.set_xlabel('Epoch', fontsize=14)\n",
        "ax.set_ylabel('Loss', fontsize=14)\n",
        "ax = plt.subplot(2, 5, 2)\n",
        "ax.plot(weights_1_history[:,0,0])\n",
        "ax.set_xlabel('Epoch', fontsize=14)\n",
        "ax.set_ylabel('$c_1$', fontsize=14)\n",
        "ax = plt.subplot(2, 5, 3)\n",
        "ax.plot(weights_1_history[:,0,1])\n",
        "ax.set_xlabel('Epoch', fontsize=14)\n",
        "ax.set_ylabel('$c_2$', fontsize=14)\n",
        "ax = plt.subplot(2, 5, 4)\n",
        "ax.plot(weights_1_history[:,1,0])\n",
        "ax.set_xlabel('Epoch', fontsize=14)\n",
        "ax.set_ylabel('$W_{11}$', fontsize=14)\n",
        "ax = plt.subplot(2, 5, 5)\n",
        "ax.plot(weights_1_history[:,1,1])\n",
        "ax.set_xlabel('Epoch', fontsize=14)\n",
        "ax.set_ylabel('$W_{12}$', fontsize=14)\n",
        "ax = plt.subplot(2, 5, 6)\n",
        "ax.plot(weights_1_history[:,2,0])\n",
        "ax.set_xlabel('Epoch', fontsize=14)\n",
        "ax.set_ylabel('$W_{21}$', fontsize=14)\n",
        "ax = plt.subplot(2, 5, 7)\n",
        "ax.plot(weights_1_history[:,2,1])\n",
        "ax.set_xlabel('Epoch', fontsize=14)\n",
        "ax.set_ylabel('$W_{22}$', fontsize=14)\n",
        "ax = plt.subplot(2, 5, 8)\n",
        "ax.plot(weights_2_history[:,0,0])\n",
        "ax.set_xlabel('Epoch', fontsize=14)\n",
        "ax.set_ylabel('$b$', fontsize=14)\n",
        "ax = plt.subplot(2, 5, 9)\n",
        "ax.plot(weights_2_history[:,1,0])\n",
        "ax.set_xlabel('Epoch', fontsize=14)\n",
        "ax.set_ylabel('$w_1$', fontsize=14)\n",
        "ax = plt.subplot(2, 5, 10)\n",
        "ax.plot(weights_2_history[:,2,0])\n",
        "ax.set_xlabel('Epoch', fontsize=14)\n",
        "ax.set_ylabel('$w_2$', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lVModDozl3CF",
        "colab": {}
      },
      "source": [
        "print(m.weights_1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AbgcccxHoKjC",
        "colab": {}
      },
      "source": [
        "print(m.weights_2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nePskL-toOUq",
        "colab": {}
      },
      "source": [
        "z_1, a_1, z_2, a_2 = m.forward(X)\n",
        "print(a_2)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}